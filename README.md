# Abschlussarbeit CAS MAZ Datenjournalismus
"Wem gehören die börsenkotierten Schweizer Unternehmen?" 
Eine Annäherung an die Besitzverhältnisse von Schweizer Unternehmen im UBS 100 Index und weiteren.

_Ausgangsthese

_Einschätzung von Aufwand/Ertrag vor Beginn des Projektes

_Bezeichnung des Knackpunkts des Projektes

## Briefing-Gespräch:
Mit Julian Chan, Mediensprecher SIX Group:
"Knackpunkte sind grundsätzlich folgende: Es sind einfach die neuesten Meldungen, aber das heisst nicht, dass sie aktuell sind. Denn Meldungen müssen nur gemacht werden, wenn eine der Schwellen, also bei 3 oder 5 Prozent oder darüber, über oder unterschritten wird. Wenn sich die Anteile zwischen den Schwellen verschieben, braucht es keine Meldung. Ausserdem werden die Meldungen von den Firmen selbst verfasst, wir publizieren sie nur. Das heisst, sie sind auch nicht einheitlich."
"Die Daten können wir leider nicht zur Verfügung stellen, wir haben sie nicht so strukturiert."

## Datensatz:
Der Datensatz wird mittels scraping der Webseite "bedeutende Aktionäre" der SIX Group, der Schweizer Börse, beschafft. Zum Abgleich und zur Auswahl der Firmen wird auf weitere Datensätze der SIX Group zugegriffen, insbesondere für die Indizes UBS 100, SMI und SMI Expanded. Diese können auf der Webseite der SIX als Excel heruntergeladen werden. Die Quelle für diese Arbeit ist also durchgehend die SIX Group. 

_Datensatz (auch bereits strukturierte Daten können verwendet werden)

_Programmiercode

_Arbeitsprotokoll (Was hast du wann weshalb gemacht?)

_je nach gewählter Variante 1,2 oder 3: Endprodukt, Skizze des weiteren
Vorgehens oder Protokoll des Scheiterns




| Tätigkeit                                                          | Datum | Zeitaufwand |
|--------------------------------------------------------------------|-------|-------------|
| Erste Exploration Datenbank, verschiedene Herangehensweisen testen | 28.1. | 8h          |
| Recherche: Papers studieren, Recherchegespräch SIX                 | 07.2. | 4h          |
| Scraper schreiben                                                  | 12.2. | 10h         |
| Scraper schreiben                                                  | 13.2. | 10h         |
| Scraper Troubleshooting                                            | 14.2. | 10h         |
| Scraper finalisieren                                               | 15.2. | 5h          |
| Daten scrapen                                                      | 15.2. | 2h          |
| Daten putzen, manuell                                              | 15.2. | 1h          |
| Pandas Auswertung                                                  | 15.2. | 2h          |
| Pandas, weitere Auswertungen                                       | 16.2. | 6h          |
| Text schreiben                                                     | 16.2. | 4h          |
| Repository erstellen                                               | 16.2. | 2h          |
