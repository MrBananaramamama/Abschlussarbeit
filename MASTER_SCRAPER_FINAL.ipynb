{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABB Ltd.',\n",
       " 'Adecco Group AG',\n",
       " 'Alcon Inc.',\n",
       " 'Allreal Holding AG',\n",
       " 'ALSO Holding AG',\n",
       " 'ams AG',\n",
       " 'ARYZTA AG',\n",
       " 'Bâloise Holding AG',\n",
       " 'Barry Callebaut AG',\n",
       " 'Banque Cantonale Vaudoise',\n",
       " 'BELIMO Holding AG',\n",
       " 'BKW AG',\n",
       " 'Bossard Holding AG',\n",
       " 'Bucher Industries AG',\n",
       " 'Burckhardt Compression Holding AG',\n",
       " 'Cembra Money Bank AG',\n",
       " 'Clariant AG',\n",
       " 'Comet Holding AG',\n",
       " 'Conzzeta AG',\n",
       " 'Credit Suisse Group AG',\n",
       " 'Dätwyler Holding AG',\n",
       " 'DKSH Holding AG',\n",
       " 'dormakaba Holding AG',\n",
       " 'DUFRY AG',\n",
       " 'Emmi AG',\n",
       " 'Ems-Chemie Holding AG',\n",
       " 'Georg Fischer AG',\n",
       " 'Flughafen Zürich AG',\n",
       " 'Forbo Holding AG',\n",
       " 'Galenica AG',\n",
       " 'GAM Holding AG',\n",
       " 'Geberit AG',\n",
       " 'Givaudan SA',\n",
       " 'Helvetia Holding AG',\n",
       " 'Huber+Suhner AG',\n",
       " 'Implenia AG',\n",
       " 'INFICON Holding AG',\n",
       " 'Interroll Holding AG',\n",
       " 'Intershop Holding AG',\n",
       " 'Julius Bär Gruppe AG',\n",
       " 'Jungfraubahn Holding AG',\n",
       " 'Kardex AG',\n",
       " 'Komax Holding AG',\n",
       " 'Kühne + Nagel International AG',\n",
       " 'LafargeHolcim Ltd',\n",
       " 'Landis+Gyr Group AG',\n",
       " 'LEM Holding SA',\n",
       " 'Liechtensteinische Landesbank AG',\n",
       " 'Logitech International S.A.',\n",
       " 'Lonza Group AG',\n",
       " 'Luzerner Kantonalbank',\n",
       " 'Mobimo Holding AG',\n",
       " 'Nestlé AG',\n",
       " 'Novartis AG',\n",
       " '\"OC Oerlikon Corporation AG',\n",
       " ' Pfäffikon\"',\n",
       " 'Pargesa Holding SA',\n",
       " 'Partners Group Holding AG',\n",
       " 'PSP Swiss Property AG',\n",
       " 'Roche Holding AG',\n",
       " 'Schindler Holding AG',\n",
       " 'Schweiter Technologies AG',\n",
       " 'SFS Group AG',\n",
       " 'SGS SA',\n",
       " 'Siegfried Holding AG',\n",
       " 'Sika AG',\n",
       " 'SoftwareONE Holding AG',\n",
       " 'Sonova Holding AG',\n",
       " 'St.Galler Kantonalbank AG',\n",
       " 'Stadler Rail AG',\n",
       " 'Straumann Holding AG',\n",
       " 'Sulzer AG',\n",
       " 'Sunrise Communications Group AG',\n",
       " 'Swiss Life Holding AG',\n",
       " 'Swiss Prime Site AG',\n",
       " 'Swiss Re AG',\n",
       " 'Swisscom AG',\n",
       " 'Tecan Group AG',\n",
       " 'Temenos AG',\n",
       " 'u-blox Holding AG',\n",
       " 'UBS Group AG',\n",
       " 'Valiant Holding AG',\n",
       " 'Valora Holding AG',\n",
       " 'VAT Group AG',\n",
       " 'Vifor Pharma AG',\n",
       " 'Vontobel Holding AG',\n",
       " 'VZ Holding AG',\n",
       " 'Zuger Kantonalbank AG',\n",
       " 'Zurich Insurance Group AG',\n",
       " '']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('UBS_100_korrigiert_FINAL.csv', 'r')\n",
    "liste = file.read()\n",
    "Comp_List = liste.replace('\\n', '').split(',')\n",
    "Comp_List\n",
    "\n",
    "#Hier gilt es anzumerken, dass einige Firmen fehlen, weil es keine Besitzstandsmeldungen gibt und auch Doppel-Einträge.\n",
    "#Insgesamt resultieren darum nur 88 Firmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-1e604f25c212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m#gelöst mit loop für column?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m#SOLVED!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnteil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mdf2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnteil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/erstesVE/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(self, level, dropna)\u001b[0m\n\u001b[1;32m   6243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstack_multiple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/erstesVE/lib/python3.7/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(frame, level, dropna)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;31m# we concatenate instead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mdtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Dieses Skript greift die Daten aus der Datenbank ab und erstellt pro Firma ein CSV.\n",
    "#Herausforderung waren insbesondere:\n",
    "    #Ungeputzte Datenbank, es hatte im HTML immer noch scheinbar unzusammenhängende Einträge drin\n",
    "    #schlecht geführte Datenbank: Die Firmen erstellen die Einträge selber, kaum Namens-, Struktur-, geschweige denn Adresskonventionen.\n",
    "    #clickbare Untermenus, welche weitere Informationen liefern: Problem hier, Chrome Browser wurde als automatisiert erkannt und gestoppt. Lösung, Firefox(hiess aber auch, Code vielerorts anpassen, da sie nicht gleich funktionieren.)\n",
    "\n",
    "#erster Teil des Loops greift die Besitzverhältnisse pro Firma ab und erstellt ersten Teil der DB.\n",
    "\n",
    "for elem in Comp_List:\n",
    "\n",
    "    \n",
    "    driver = webdriver.Firefox(executable_path='/Users/fabianvonallmen/.wdm/drivers/geckodriver/v0.26.0/macos/geckodriver')\n",
    "    \n",
    "    driver.delete_all_cookies()\n",
    "    \n",
    "    driver.get('https://www.six-exchange-regulation.com/de/home/publications/significant-shareholders.html')\n",
    "    \n",
    "    #mal aktuelle beteiligungen anclicken\n",
    "    driver.find_element_by_class_name('checkbox').click()\n",
    "    #seriously? nur weil das oben war, nope. leider nein. \n",
    "    #das menu mal anwählen\n",
    "    select = Select(driver.find_element_by_name('NotificationSubmitterId'))\n",
    "    \n",
    "    #jede Firma anwählen mit der Comp_List\n",
    "\n",
    "    select.select_by_visible_text(elem)\n",
    "\n",
    "    #UNTEN, alte Chrome-Befehle, der bei Firefox nicht funktioniert. Zur Sicherheit drin lassen, falls sich wieder was ändert in den Versionen.\n",
    "    #enter = driver.find_element_by_name('NotificationSubmitterId').send_keys(Keys.ENTER)\n",
    "    \n",
    "    #driver.execute_script(\"window.scrollTo(1000, 400)\") \n",
    "    #target = driver.find_element_by_class_name('clearfix')\n",
    "    #target.location_once_scrolled_into_view\n",
    "\n",
    "    #driver.find_element_by_class_name('pull-right').click()\n",
    "    \n",
    "    ####AND HERE, FIREFOX SPECIAL, PUSH THE BUTTON!\n",
    "    time.sleep(3)\n",
    "    #click 'Resultate anzeigen'\n",
    "    driver.find_element_by_xpath(\"/html/body/div/div[5]/div[3]/div[1]/div[3]/div[1]/form/div/div[3]/button/span\").click()\n",
    "    \n",
    "\n",
    "    #EIGENTLICH KÖNNTE MAN DAS STREICHEN, DIREKT WEITER OHNE ABSPEICHERN...zur Sicherheit lassen wirs drin.\n",
    "    page = driver.page_source.encode('utf-8')\n",
    "    with open(elem+'.htm', \"wb+\") as file:\n",
    "        file.write(page)\n",
    "        file.close()\n",
    "        \n",
    "    time.sleep(3)\n",
    "    \n",
    "    #und gleich wieder öffnen und beautifulsoupen\n",
    "    file = open(elem+'.htm', 'r')\n",
    "    text = file.read()\n",
    "    abbh = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    #gleich suchradius definieren, als Grundlage für weiterere Arbeit\n",
    "    tabelle = abbh.find_all('div', {'class': 'table-scroll'})\n",
    "    \n",
    "    #jetzt eine column finden um diese dann später einzeln durchzulaufen!\n",
    "\n",
    "    column = tabelle[0].find_all('tr', {'class': 'itf-mark-row-template even'})\n",
    "    column2 = tabelle[0].find_all('tr', {'class': 'itf-mark-row-template odd'})\n",
    "    \n",
    "    #das ganze mit odd UND even!!! Site so strukturiert, machts auch nicht einfacher. \n",
    "    #problem: offenbar sind da noch weitere Daten im HTML versteckt. Das ist gspässig und macht die sache nicht einfacher!\n",
    "    #wtf?!\n",
    "    #vielleicht müsste man die, die nicht dem suchbegriff \"ABB\" entsprechen mal schön löschen! das machen wir mit dem df.df issuer == ABB Ltd\n",
    "\n",
    "    ergebnis = []\n",
    "        \n",
    "    for elem2 in column:\n",
    "    \n",
    "                DS = elem2.find('div', {'data-itf-if-filled': \"ShareholderNames\"}).get_text('/')\n",
    "        #und kill 'mehr anzeigen'\n",
    "                DS = DS.replace('/ Mehr anzeigen', '')\n",
    "                Issuer = elem2.find('td', {'data-itf-inject': \"NotificationSubmitter\"}).text\n",
    "                BO = elem2.find('div', {'data-itf-inject': \"BeneficialNames\"}).get_text('/')\n",
    "                BO = BO.replace('/ Mehr anzeigen', '')\n",
    "                Anteil = elem2.find('span', {'data-itf-inject': \"PurchaseTotalVotingRate\"}).text\n",
    "    \n",
    "                minidict = {'Issuer': Issuer, \n",
    "                            'BO': BO,\n",
    "                            'DS': DS, \n",
    "                            'Anteil': Anteil}\n",
    "\n",
    "\n",
    "                ergebnis.append(minidict)\n",
    "                \n",
    "    for elem3 in column2:\n",
    "        \n",
    "    \n",
    "                DS = elem3.find('div', {'data-itf-if-filled': \"ShareholderNames\"}).get_text('/')\n",
    "                DS = DS.replace('/ Mehr anzeigen', '')\n",
    "                Issuer = elem3.find('td', {'data-itf-inject': \"NotificationSubmitter\"}).text\n",
    "                BO = elem3.find('div', {'data-itf-inject': \"BeneficialNames\"}).get_text('/')\n",
    "                BO = BO.replace('/ Mehr anzeigen', '')\n",
    "                Anteil = elem3.find('span', {'data-itf-inject': \"PurchaseTotalVotingRate\"}).text\n",
    "        \n",
    "        \n",
    "            \n",
    "                minidict = {'Issuer': Issuer, \n",
    "                            'BO': BO,\n",
    "                            'DS': DS, \n",
    "                            'Anteil': Anteil}\n",
    "        \n",
    "                ergebnis.append(minidict)\n",
    "        \n",
    "           \n",
    "            \n",
    "            \n",
    "                df = pd.DataFrame(ergebnis)\n",
    "                \n",
    "                #eben, rest des gschmöis raus!\n",
    "                df = df[df.Issuer == elem]\n",
    "                \n",
    "#PROBLEM:\n",
    "                #elem and Issuer are not in synch!\n",
    "                \n",
    "            #something is not in sync here. It only gets ABB. Oh, oh, is the rest of the Website different? NOPE\n",
    "            #must be a loop logic problem\n",
    "            #nimmt nur das erste element, hmmm, bleibt stecken.\n",
    "            #gelöst mit loop für column?\n",
    "#SOLVED!!\n",
    "                df1= pd.DataFrame(pd.DataFrame(df.BO.str.split('/').tolist(), index =df.Anteil).stack())\n",
    "    \n",
    "                df2= pd.DataFrame(pd.DataFrame(df.DS.str.split('/').tolist(), index =df.Anteil).stack())\n",
    "    \n",
    "                newdf = pd.merge(df1, df2, left_index=True, right_index=True, how='outer')\n",
    "                newdf = newdf.dropna()\n",
    "                newdf = newdf.replace('\\n', np.nan)\n",
    "                #columns umbenennen\n",
    "                newdf = newdf.rename(columns={\"0_x\": \"BO\", \"0_y\": \"DS\"})\n",
    "                #Name der Firma in neuer Column\n",
    "                newdf.insert(0, 'Company', elem)\n",
    "\n",
    "    \n",
    "                #abspeichern für die konvertierung, sehr unelegant...\n",
    "                newdf.to_csv(r'zwischen_'+elem +'.csv')\n",
    "    \n",
    "                #eben, öffnen aber zum glück nun ohne multiindex \n",
    "                dfliste_ohne = pd.read_csv('zwischen_'+elem +'.csv')\n",
    "        \n",
    "                #die folgende geniale funktion teilt die anteile für jede person einzeln auf, so dass man genau sehen kann, wer wie viel anteil hat. \n",
    "                #Achtung: Das kann aber nur eine statistische Annäherung sein, da nicht klar ist, ob die Anteile gleichwertig auf die verschiedenen Personen verteilt sind. \n",
    "                #Kann man aber begründen und machen, insbesondere um die Ländereinteilung mit Infos zu füttern. \n",
    "                \n",
    "                news = (dfliste_ohne['Anteil'].str.replace('%', '').astype(float) / \n",
    "                      dfliste_ohne.groupby('Anteil')['BO'].\n",
    "                      transform('count')).astype(str) + '%'\n",
    "                #nach dem teilen wieder zusammenführen, divide et impera!\n",
    "                dfliste_ohne['Anteil'] = news\n",
    "                #ACHTUNG: HIER ZWISCHENSPEICHERN?\n",
    "                dfliste_ohne.to_csv(elem+'_OHNE_Länder.csv')\n",
    "                \n",
    "        \n",
    "                #scroll down\n",
    "                target = driver.find_element_by_class_name('table-scroll')\n",
    "                target.location_once_scrolled_into_view\n",
    "                lst = driver.find_elements_by_class_name('fa.fa-info-circle.fa-2x')\n",
    "    \n",
    "    \n",
    "                time.sleep(2)\n",
    "                #####\n",
    "        \n",
    "#JETZT kommt der Unterlink mit den Ländern teil! \n",
    "#hier die verschiedenen einträge abclicken und ins untermenu rein. \n",
    "    Unterliste = []\n",
    "\n",
    "    for clickloop in lst:\n",
    "        try:                \n",
    "            clickloop.click()\n",
    "            time.sleep(2)\n",
    "            #hier alles machen (INPUT 1)\n",
    "            \n",
    "            page = driver.page_source.encode('utf-8')\n",
    "        \n",
    "            #die ganze csv speicherei lasse ich hier mal bleiben, das mach ich besser mit append. \n",
    "        \n",
    "            bs = BeautifulSoup(page, 'html.parser')    \n",
    "                        #########\n",
    "                        #muss ich das erst abspeichern, oder kann ich es einfach appenden? source encode und weiter im text.\n",
    "                        #TRY!!!\n",
    "        \n",
    "                        #alle infos in dieser linie, kommen als schlauch raus, werden mit / in die jeweiligen Einträge geteilt. \n",
    "            adr = bs.find('td', {'data-itf-inject': 'BeneficialAddrs'}).get_text('/')\n",
    "            adr = adr.split('/')\n",
    "        \n",
    "        \n",
    "                        #alle infos in dieser linie, kommen als schlauch raus, werden mit / in die jeweiligen Einträge geteilt. \n",
    "            s = pd.Series(adr)\n",
    "                        #von hier in Container speichern:\n",
    "            Unterliste.append(s)\n",
    "        \n",
    "                        #ZURÜCK UND WIEDER VON VORNE \n",
    "            driver.back()\n",
    "            time.sleep(6)\n",
    "        except Exception:\n",
    "             pass\n",
    "            \n",
    "                #master =  master.merge(lst,  how= 'outer') #master muss schon definiert sein\n",
    "                \n",
    "                ###\n",
    "                    #all in one!\n",
    "    U = pd.DataFrame(Unterliste).stack(dropna=True).str.split(pat = \",\", expand=True)\n",
    "                #make non to NaN\n",
    "    U.fillna(value=pd.np.nan, inplace=True)\n",
    "\n",
    "    this = pd.DataFrame(U.ffill(axis=1).iloc[:, -1])\n",
    "\n",
    "                #holt den ersten Eintrag, dieser jeweils Name oder Firma. \n",
    "    this2 = U.iloc[:, [0]]\n",
    "\n",
    "                #zusammenfügen, damit mit 0 Column verbunden werden kann. Eventuell Name ändern. \n",
    "    df_country = pd.concat([this2, this], axis=1, join='outer')\n",
    "\n",
    "                #rename it, damit das verbunden werden kann mit master df\n",
    "    df_country= df_country.rename(columns={0: \"BO\", df_country.columns[-1]: \"Country\"})\n",
    "                \n",
    "                #mal zwischenspeichern\n",
    "    df_country.to_csv(elem+'__Länder.csv')\n",
    "    \n",
    "    #AND NOW:\n",
    "    #ALLES ZUSAMMENFÜHREN UND ABSPEICHERN!!\n",
    "    master =  dfliste_ohne.merge(df_country,  how= 'outer')\n",
    "    master.to_csv(elem+'_MASTER.csv')\n",
    "        \n",
    "    driver.close()\n",
    "            \n",
    "           \n",
    "             \n",
    "                            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = driver.page_source.encode('utf-8')\n",
    "        \n",
    "            #die ganze csv speicherei lasse ich hier mal bleiben, das mach ich besser mit append. \n",
    "        \n",
    "bs = BeautifulSoup(page, 'html.parser')    \n",
    "                        #########\n",
    "                        #muss ich das erst abspeichern, oder kann ich es einfach appenden? source encode und weiter im text.\n",
    "                        #TRY!!!\n",
    "        \n",
    "                        #alle infos in dieser linie, kommen als schlauch raus, werden mit / in die jeweiligen Einträge geteilt. \n",
    "adr = bs.find('td', {'data-itf-inject': 'BeneficialAddrs'}).get_text('/')\n",
    "adr = adr.split('/')\n",
    "        \n",
    "        \n",
    "                        #alle infos in dieser linie, kommen als schlauch raus, werden mit / in die jeweiligen Einträge geteilt. \n",
    "s = pd.Series(adr)\n",
    "                        #von hier in Container speichern:\n",
    "Unterliste.append(s)\n",
    "        \n",
    "                        #ZURÜCK UND WIEDER VON VORNE \n",
    "driver.back()\n",
    "time.sleep(6)\n",
    "       \n",
    "            \n",
    "                        #master =  master.merge(lst,  how= 'outer') #master muss schon definiert sein\n",
    "                \n",
    "                ###\n",
    "                    #all in one!\n",
    "U = pd.DataFrame(Unterliste).stack(dropna=True).str.split(pat = \",\", expand=True)\n",
    "                #make non to NaN\n",
    "U.fillna(value=pd.np.nan, inplace=True)\n",
    "\n",
    "this = pd.DataFrame(U.ffill(axis=1).iloc[:, -1])\n",
    "\n",
    "                #holt den ersten Eintrag, dieser jeweils Name oder Firma. \n",
    "this2 = U.iloc[:, [0]]\n",
    "\n",
    "                #zusammenfügen, damit mit 0 Column verbunden werden kann. Eventuell Name ändern. \n",
    "df_country = pd.concat([this2, this], axis=1, join='outer')\n",
    "\n",
    "            #rename it, damit das verbunden werden kann mit master df\n",
    "df_country= df_country.rename(columns={0: \"BO\", df_country.columns[-1]: \"Country\"})\n",
    "                \n",
    "                #mal zwischenspeichern\n",
    "df_country.to_csv(elem+'__Länder.csv')\n",
    "    \n",
    "    #AND NOW:\n",
    "    #ALLES ZUSAMMENFÜHREN UND ABSPEICHERN!!\n",
    "master =  dfliste_ohne.merge(df_country,  how= 'outer')\n",
    "master.to_csv(elem+'_MASTER.csv')\n",
    "        \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('UBS 100 Index.csv', 'r')\n",
    "liste = file.read()\n",
    "liste = liste.replace('\\n', '').split(',')\n",
    "liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Comp_List = ('ABB Ltd', 'Actelion Ltd')\n",
    "\n",
    "for elem in Comp_List:\n",
    "    #unbedingt fehlertoleranz einbauen, falls ein wort falsch in Comp_List oder Einträge leer!\n",
    "    try: \n",
    "\n",
    "    \n",
    "        driver = webdriver.Firefox(executable_path='/Users/fabianvonallmen/.wdm/drivers/geckodriver/v0.26.0/macos/geckodriver')\n",
    "    \n",
    "        driver.delete_all_cookies()\n",
    "    \n",
    "        driver.get('https://www.six-exchange-regulation.com/de/home/publications/significant-shareholders.html')\n",
    "    \n",
    "        #mal aktuelle beteiligungen anclicken\n",
    "        driver.find_element_by_class_name('checkbox').click()\n",
    "        #seriously? nur weil das oben war, nope. leider nein. \n",
    "        #das menu mal anwählen\n",
    "        select = Select(driver.find_element_by_name('NotificationSubmitterId'))\n",
    "    \n",
    "        #jede Firma anwählen mit der Comp_List\n",
    "        select.select_by_visible_text(elem)\n",
    "        #enter = driver.find_element_by_name('NotificationSubmitterId').send_keys(Keys.ENTER)\n",
    "    \n",
    "        #driver.execute_script(\"window.scrollTo(1000, 0)\") \n",
    "        #target = driver.find_element_by_class_name('clearfix')\n",
    "        #target.location_once_scrolled_into_view\n",
    "\n",
    "        #driver.find_element_by_class_name('pull-right').click()\n",
    "    \n",
    "        ####AND HERE, FIREFOX SPECIAL, PUSH THE BUTTON!\n",
    "        time.sleep(3)\n",
    "        #click 'Resultate anzeigen'\n",
    "        driver.find_element_by_xpath(\"/html/body/div/div[5]/div[3]/div[1]/div[3]/div[1]/form/div/div[3]/button/span\").click()\n",
    "    \n",
    "        time.sleep(1)\n",
    "    \n",
    "    \n",
    "        #EIGENTLICH KÖNNTE MAN DAS STREICHEN, DIREKT WEITER OHNE ABSPEICHERN...\n",
    "        page = driver.page_source.encode('utf-8')\n",
    "        with open(elem+'.htm', \"wb+\") as file:\n",
    "            file.write(page)\n",
    "            file.close()\n",
    "        \n",
    "        time.sleep(3)\n",
    "    \n",
    "        ##driver.close()\n",
    "    \n",
    "        #und gleich wieder öffnen und beautifulsoupen\n",
    "        file = open(elem+'.htm', 'r')\n",
    "        text = file.read()\n",
    "        abbh = BeautifulSoup(text, 'html.parser')\n",
    "        \n",
    "        #gleich suchradius definieren, als Grundlage für weiterere Arbeit\n",
    "        tabelle = abbh.find_all('div', {'class': 'table-scroll'})\n",
    "    \n",
    "        #jetzt eine column finden um diese dann später einzeln durchzulaufen!\n",
    "        #column\n",
    "            \n",
    "        #jetzt eine column finden um diese dann später einzeln durchzulaufen!\n",
    "        #column\n",
    "        #könnte man diesen Schritt nicht einfach, respektive den tabellenschritt einfach auslassen?\n",
    "        column = tabelle[0].find_all('tr', {'class': 'itf-mark-row-template even'})\n",
    "        column2 = tabelle[0].find_all('tr', {'class': 'itf-mark-row-template odd'})\n",
    "    \n",
    "        #das ganze mit odd UND even!!!\n",
    "        #ausserdem werden die mehrfacheinträge schon mal mit \n",
    "        #problem: offenbar sind da noch weitere Daten im HTML versteckt. Das ist gspässig und macht die sache nicht einfacher!\n",
    "        #wtf?!\n",
    "        #vielleicht müsste man die, die nicht dem suchbegriff \"ABB\" entsprechen mal schön löschen! das machen wir mit dem df.df issuer == ABB Ltd\n",
    "\n",
    "        ergebnis = []\n",
    "    except Exception:\n",
    "        pass\n",
    "        \n",
    "        for elem2 in column:\n",
    "            try:\n",
    "    \n",
    "                    DS = elem2.find('div', {'data-itf-if-filled': \"ShareholderNames\"}).get_text('/')\n",
    "        #und kill 'mehr anzeigen'\n",
    "                    DS = DS.replace('/ Mehr anzeigen', '')\n",
    "                    Issuer = elem2.find('td', {'data-itf-inject': \"NotificationSubmitter\"}).text\n",
    "                    BO = elem2.find('div', {'data-itf-inject': \"BeneficialNames\"}).get_text('/')\n",
    "                    BO = BO.replace('/ Mehr anzeigen', '')\n",
    "                    Anteil = elem2.find('span', {'data-itf-inject': \"PurchaseTotalVotingRate\"}).text\n",
    "    \n",
    "                    minidict = {'Issuer': Issuer, \n",
    "                            'BO': BO,\n",
    "                            'DS': DS, \n",
    "                            'Anteil': Anteil}\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "                ergebnis.append(minidict)\n",
    "                \n",
    "        for elem3 in column2:\n",
    "            try:\n",
    "        \n",
    "                    DS = elem3.find('div', {'data-itf-if-filled': \"ShareholderNames\"}).get_text('/')\n",
    "                    DS = DS.replace('/ Mehr anzeigen', '')\n",
    "                    Issuer = elem3.find('td', {'data-itf-inject': \"NotificationSubmitter\"}).text\n",
    "                    BO = elem3.find('div', {'data-itf-inject': \"BeneficialNames\"}).get_text('/')\n",
    "                    BO = BO.replace('/ Mehr anzeigen', '')\n",
    "                    Anteil = elem3.find('span', {'data-itf-inject': \"PurchaseTotalVotingRate\"}).text\n",
    "            \n",
    "                    minidict = {'Issuer': Issuer, \n",
    "                            'BO': BO,\n",
    "                            'DS': DS, \n",
    "                            'Anteil': Anteil}\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "                ergebnis.append(minidict)\n",
    "            \n",
    "                df = pd.DataFrame(ergebnis)\n",
    "                \n",
    "                df = df[df.Issuer == elem]\n",
    "                \n",
    "#SOLVED!!\n",
    "                #elem and Issuer are not in synch!\n",
    "                \n",
    "                 #something is not in sync here. It only gets ABB. Oh, oh, is the rest of the Website different? NOPE\n",
    "            #must be a loop logic problem\n",
    "            #nimmt nur das erste element, hmmm, bleibt stecken.\n",
    "            #gelöst mit loop für column?\n",
    "#SOLVED!!\n",
    "                df1= pd.DataFrame(pd.DataFrame(df.BO.str.split('/').tolist(), index =df.Anteil).stack())\n",
    "    \n",
    "                df2= pd.DataFrame(pd.DataFrame(df.DS.str.split('/').tolist(), index =df.Anteil).stack())\n",
    "    \n",
    "                newdf = pd.merge(df1, df2, left_index=True, right_index=True, how='outer')\n",
    "                newdf = newdf.dropna()\n",
    "                newdf = newdf.replace('\\n', np.nan)\n",
    "                #columns umbenennen\n",
    "                newdf = newdf.rename(columns={\"0_x\": \"BO\", \"0_y\": \"DS\"})\n",
    "                #Name der Firma in neuer Column\n",
    "                newdf.insert(0, 'Company', elem)\n",
    "\n",
    "    \n",
    "                #abspeichern für die konvertierung, sehr unelegant...\n",
    "                newdf.to_csv(r'zwischen_'+elem +'.csv')\n",
    "    \n",
    "                #eben, öffnen aber zum glück nun ohne multiindex \n",
    "                dfliste_ohne = pd.read_csv('zwischen_'+elem +'.csv')\n",
    "        \n",
    "                #diese geniale funktion teilt die anteile für jede person einzeln auf, so dass \n",
    "                news = (dfliste_ohne['Anteil'].str.replace('%', '').astype(float) / \n",
    "                          dfliste_ohne.groupby('Anteil')['BO'].\n",
    "                          transform('count')).astype(str) + '%'\n",
    "                    #nach dem teilen wieder zusammenführen, divide et impera!\n",
    "                dfliste_ohne['Anteil'] = news\n",
    "                    #ACHTUNG: HIER ZWISCHENSPEICHERN?\n",
    "                dfliste_ohne.to_csv(elem+'_OHNE_Länder.csv')\n",
    "    \n",
    "        \n",
    "                #JETZT kommt der Unterlink mit den Ländern teil!\n",
    "                #driver\n",
    "                \n",
    "                #just tryin if iloc works\n",
    "               \n",
    "                    \n",
    "#hier die verschiedenen einträge abclicken und ins untermenu rein. \n",
    "                \n",
    "        \n",
    "        #scroll down\n",
    "                target = driver.find_element_by_class_name('table-scroll')\n",
    "                target.location_once_scrolled_into_view\n",
    "    \n",
    "    #ABER JETZT ERST NOCH DIE UNTERMENUS LÄNDER ERSTELLEN\n",
    "                lst = driver.find_elements_by_class_name('fa.fa-info-circle.fa-2x')\n",
    "    \n",
    "    \n",
    "                time.sleep(2)\n",
    "                #####\n",
    "\n",
    "    \n",
    "        Unterliste = []\n",
    "\n",
    "    for clickloop in lst:\n",
    "        try:                \n",
    "            clickloop.click()\n",
    "            time.sleep(2)\n",
    "            #hier alles machen (INPUT 1)\n",
    "            \n",
    "            page = driver.page_source.encode('utf-8')\n",
    "        \n",
    "            #die ganze csv speicherei lasse ich hier mal bleiben, das mach ich besser mit append. \n",
    "        \n",
    "            bs = BeautifulSoup(page, 'html.parser')    \n",
    "                        #########\n",
    "                        #muss ich das erst abspeichern, oder kann ich es einfach appenden? source encode und weiter im text.\n",
    "                        #TRY!!!\n",
    "        \n",
    "                        #alle infos in dieser linie, kommen als schlauch raus, werden mit / in die jeweiligen Einträge geteilt. \n",
    "            adr = bs.find('td', {'data-itf-inject': 'BeneficialAddrs'}).get_text('/')\n",
    "            adr = adr.split('/')\n",
    "        \n",
    "        \n",
    "                        #alle infos in dieser linie, kommen als schlauch raus, werden mit / in die jeweiligen Einträge geteilt. \n",
    "            s = pd.Series(adr)\n",
    "                        #von hier in Container speichern:\n",
    "            Unterliste.append(s)\n",
    "        \n",
    "                        #ZURÜCK UND WIEDER VON VORNE \n",
    "            driver.back()\n",
    "            time.sleep(6)\n",
    "        except Exception:\n",
    "             pass\n",
    "            \n",
    "                        #master =  master.merge(lst,  how= 'outer') #master muss schon definiert sein\n",
    "                \n",
    "                ###\n",
    "                    #all in one!\n",
    "    U = pd.DataFrame(Unterliste).stack(dropna=True).str.split(pat = \",\", expand=True)\n",
    "                #make non to NaN\n",
    "    U.fillna(value=pd.np.nan, inplace=True)\n",
    "\n",
    "    this = pd.DataFrame(U.ffill(axis=1).iloc[:, -1])\n",
    "\n",
    "                #holt den ersten Eintrag, dieser jeweils Name oder Firma. \n",
    "    this2 = U.iloc[:, [0]]\n",
    "\n",
    "                #zusammenfügen, damit mit 0 Column verbunden werden kann. Eventuell Name ändern. \n",
    "    df_country = pd.concat([this2, this], axis=1, join='outer')\n",
    "\n",
    "                #rename it, damit das verbunden werden kann mit master df\n",
    "    df_country= df_country.rename(columns={0: \"BO\", df_country.columns[-1]: \"Country\"})\n",
    "                \n",
    "                #mal zwischenspeichern\n",
    "    df_country.to_csv(elem+'__Länder.csv')\n",
    "    \n",
    "    #AND NOW:\n",
    "    #ALLES ZUSAMMENFÜHREN UND ABSPEICHERN!!\n",
    "    master =  dfliste_ohne.merge(df_country,  how= 'outer')\n",
    "    master.to_csv(elem+'_MASTER.csv')\n",
    "        \n",
    "    driver.close()\n",
    "            \n",
    "           \n",
    "             \n",
    "                            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
